Note: description generated by ChatGPT.
Code examples handwritten, if provided.

# ============================================================
# Class: PCA
# ============================================================
# Principal Component Analysis (PCA) implemented fully in PyTorch.
# Provides batched fitting and GPU acceleration, without internal centering.
# 
# Features:
#   - Works with large datasets by accumulating the Gram matrix in batches.
#   - No internal centering: input data must already be mean-centered.
#   - Device-agnostic: can run seamlessly on CPU or GPU.
#   - Outputs both raw and normalized explained variances.
# 
# Args:
#   n_components (int):
#       Number of principal components to retain (must be > 0).
#   dtype (torch.dtype):
#       Data type for internal computations and stored parameters.
#       Default: torch.float32.
# 
# Attributes:
#   components_ (torch.Tensor):
#       Principal component vectors (D, K), stored as registered buffer.
#   explained_variance_ (torch.Tensor):
#       Raw eigenvalues (variances) for each principal component (K,).
#   explained_ratio_ (torch.Tensor):
#       Proportion of variance explained by each component (K,).
#   total_variance_ (torch.Tensor):
#       Total variance (sum of all eigenvalues).
#   fitted_ (bool):
#       Indicates whether the PCA has been successfully fit.
# 
# Methods:
#   fit(X: torch.Tensor, batch_size: int = 64) -> PCA:
#       Computes PCA decomposition on pre-centered data using batched Gram accumulation.
#       Args:
#         X: Input tensor (N, D), already mean-centered.
#         batch_size: Batch size for streaming computation of X^T X.
#       Returns:
#         Self (the fitted PCA instance).
# 
#   transform(X: torch.Tensor, batch_size: Optional[int] = None) -> torch.Tensor:
#       Projects new, pre-centered data onto the learned principal components.
#       Args:
#         X: Input tensor (N, D), centered the same way as data used for fitting.
#         batch_size: Optional batch size for chunked processing.
#       Returns:
#         Transformed tensor of shape (N, K).
# 
#   explained_variance_ratio() -> torch.Tensor:
#       Returns the fraction of total variance explained by each selected component.
#       Output tensor length = K, values in [0, 1].
# 
#   explained_variance_values() -> torch.Tensor:
#       Returns the raw eigenvalues (variances) associated with each component.
# 
# Usage Example:
#   >>> pca = PCA(n_components=8).to("cuda")
#   >>> pca.fit(X_train_centered, batch_size=128)
#   >>> X_proj = pca.transform(X_test_centered)
#   >>> print(pca.explained_variance_ratio())



# ============================================================
# Class: IncrementalPCA
# ============================================================
# Pure-PyTorch Incremental PCA with tensor I/O, dtype & device support.
#
# API parity (most-used parts):
#   - __init__(n_components=None, whiten=False, batch_size=None, dtype=torch.float32,
#              device=None, acc_dtype=torch.float64, eps=1e-9)
#   - fit(X), partial_fit(X), fit_transform(X), transform(X), inverse_transform(Z)
#   - get_covariance(), get_precision(), reset()
#
# Overview:
#   - Maintains exact first/second moments (S1 = sum x, S2 = sum x^T x).
#   - After each partial_fit, recomputes covariance eigenpairs.
#   - Robust, simple, and device-agnostic (CPU/GPU).
#
# Args:
#   n_components (int | None):
#       Number of principal components to retain. If None, keep all.
#   whiten (bool):
#       If True, transform() divides by sqrt(explained_variance_ + eps).
#   batch_size (int | None):
#       For fit(X): when set, streams rows in chunks of this size.
#   dtype (torch.dtype):
#       Internal model parameter dtype (default: torch.float32).
#   device (torch.device | str | None):
#       Device hosting parameters/accumulators. If None, inferred from first input.
#   acc_dtype (torch.dtype):
#       Accumulator precision for S1/S2 (default: torch.float64 for stability).
#   eps (float):
#       Numerical epsilon used in whitening.
#
# Attributes (after fit/partial_fit):
#   components_ (Tensor):             (n_components, n_features)
#   explained_variance_ (Tensor):     (n_components,)
#   explained_variance_ratio_ (Tensor):(n_components,)
#   singular_values_ (Tensor):        (n_components,)
#   mean_ (Tensor):                   (n_features,)
#   var_ (Tensor):                    (n_features,)  # per-feature variance (unbiased)
#   noise_variance_ (Tensor/float):   scalar; average discarded eigenvalues
#   n_components_ (int):              effective number of components kept
#   n_features_ (int):                number of features
#   n_samples_seen_ (int):            total samples processed
#
# Public Methods:
#   partial_fit(X: torch.Tensor) -> IncrementalPCA
#       Incrementally update the model with a batch X (n_samples, n_features).
#       Refreshes components_ and statistics after the update.
#
#   fit(X: torch.Tensor) -> IncrementalPCA
#       Fit from a 2D tensor. If batch_size is set, streams X by rows.
#
#   fit_transform(X: torch.Tensor) -> torch.Tensor
#       Fit the model and return X projected to principal components.
#
#   transform(X: torch.Tensor) -> torch.Tensor
#       Project X onto the principal component space. If whiten=True,
#       divides by sqrt(explained_variance_ + eps). Returns on X.device/dtype.
#
#   inverse_transform(Z: torch.Tensor) -> torch.Tensor
#       Reconstruct from component space back to feature space. Returns on Z.device/dtype.
#
#   get_covariance() -> torch.Tensor
#       Approximate feature covariance:
#         C â‰ˆ components_.T diag(explained_variance_) components_ + noise_variance_ * I
#
#   get_precision() -> torch.Tensor
#       Return inverse covariance (precision) via pseudo-inverse for stability.
#
#   reset() -> None
#       Reset the estimator to the initial, unfitted state (clears moments and model params).
#
# Usage:
#   ipca = IncrementalPCA(n_components=128, whiten=True, batch_size=2048). # init
#   ipca.fit(X_train)                          # or stream with partial_fit on mini-batches
#   Z = ipca.transform(X_val)                  # project
#   X_rec = ipca.inverse_transform(Z)          # reconstruct
#   C = ipca.get_covariance()                  # covariance estimate
#   P = ipca.get_precision()                   # precision matrix

