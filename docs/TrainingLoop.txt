Note: description generated by ChatGPT.
Code examples handwritten, if provided.

Gotchas & best practices
- If pin_memory=True in your DataLoader, non_blocking=True is good; if not, set non_blocking=False.
- Ensure tensors added to TensorBatchStorage have matching shapes on all dims except cat_dim.
- Remember: the final partial window (< K batches) isn’t logged that epoch—adjust log_every_k to your dataset size.
- keep_outputs=False saves RAM but makes compute_metrics receive (None, None).
- scheduler is called (stepped) after every epoch.

# ============================================================
# Class: TrainingLoop
# ============================================================
# A flexible and extensible training pipeline wrapper for PyTorch.
# Supports checkpointing, metric computation, loss tracking,
# and batched GPU→CPU logging.
#
# Args:
#   model (nn.Module): Model to be trained.
#   optimizer (torch.optim.Optimizer): Optimizer for parameter updates.
#   criterion (Callable): Loss function.
#   device (torch.device): Target device (e.g., 'cuda' or 'cpu').
#   epochs (int): Number of epochs to train.
#   train_loop_constructor (Callable): Function returning a tqdm iterator for training data.
#   test_loop_constructor (Callable): Function returning a tqdm iterator for test data.
#   log_every_k (int): Batching frequency for CPU transfers (reduces I/O cost).
#   recent_memory (int): Number of recent batches used for running average.
#   scheduler (torch.optim.lr_scheduler._LRScheduler, optional): Learning rate scheduler.
#   checkpoint_path (str, optional): Path to save checkpoints each epoch.
#   best_path (str, optional): Path to save the best-performing model.
#   non_blocking (bool): Enables non-blocking CUDA transfers.
#   keep_outputs (bool): Whether to retain all outputs/targets for metric computation.
#
# Hooks to Override:
#   train_batch(data) -> (loss, outputs, targets):
#       Defines a single training step. Can be customized for complex batches.
#
#   test_batch(data) -> (loss, outputs, targets):
#       Defines a single evaluation step. It is supposed to be executed under torch.inference_mode() or torch.no_grad(), but you must do this manually.
#
#   compute_metrics(outputs, targets) -> Any:
#       Computes validation metrics at epoch end. Called with (None, None) if keep_outputs=False.
#
#   print_epoch_results(epoch, train_loss, train_metrics, test_loss, test_metrics) -> None:
#       Defines how results are displayed after each epoch.
#
#   update_pbar(loop, mode, epoch, avg_loss, recent_loss) -> None:
#       Updates the progress bar display for train/test phases.
#
#   quantify(test_loss, test_metrics) -> float:
#       Produces a single scalar score used to track the "best" model.
#       Lower values are considered better.
#       By default: return test_loss
#
#   post_epoch(history: List[dict]) -> Optional[bool]:
#       Called at the end of every epoch. Can implement early stopping.
#       Return True to stop training.
#
# Internal Methods:
#   load_checkpoint(path): Loads model, optimizer, scheduler states, and history.
#   save_checkpoint(path): Saves model, optimizer, scheduler states, and history.
#   _train_step(epoch): Executes one training epoch.
#   _test_step(epoch): Executes one evaluation epoch.
#   run(resume=True): Main training entry point.
#
# Usage:
#   loop = TrainingLoop(model, optimizer, criterion, device, epochs,
#                       lambda: tqdm(train_loader), lambda: tqdm(test_loader))
#   loop.run()


TODO example